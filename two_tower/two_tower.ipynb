{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import clip\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 1: LOAD DATA ====\n",
    "# Load clothing dataset\n",
    "clothing_excel_path = \"C:/Users/megdy/Desktop/stat 4830/STAT-4830-vllm-project/two_tower/clothing_data.xlsx\"\n",
    "df_items = pd.read_excel(clothing_excel_path)\n",
    "df_items.drop([\"image\", \"average\"], axis=1, inplace=True, errors='ignore')  # Drop unused columns\n",
    "\n",
    "# Load user ratings dataset\n",
    "ratings_excel_path = \"C:/Users/megdy/Desktop/stat 4830/STAT-4830-vllm-project/two_tower/rating_data.xlsx\"\n",
    "df_ratings = pd.read_excel(ratings_excel_path)\n",
    "df_ratings.drop([\"image\", \"average\"], axis=1, inplace=True, errors='ignore')  # Drop unused columns\n",
    "df_ratings = df_ratings.set_index(df_items.index)  # Align indices with clothing dataset\n",
    "\n",
    "# Ensure all ratings are numeric\n",
    "df_ratings = df_ratings.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 2: SET IMPLICIT 10 FOR USERS WHO LIKED THE ITEM ====\n",
    "for idx, row in df_items.iterrows():\n",
    "    liked_user = row[\"user\"].strip().lower()  # Get user who originally liked the item\n",
    "    for col in df_ratings.columns:\n",
    "        if col.strip().lower() == liked_user:\n",
    "            df_ratings.at[idx, col] = 10  # Assign rating of 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megdy\\AppData\\Local\\Temp\\ipykernel_55380\\1632367810.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_ratings_thresholded = df_ratings.applymap(lambda x: 1 if x >= 7 else -1)\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 3: THRESHOLD RATINGS ====\n",
    "df_ratings = df_ratings.applymap(lambda x: 1 if x >= 7 else -1)\n",
    "df_ratings = df_ratings.reindex(df_items.index)  # Ensure index alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 4: CREATE FEATURE REPRESENTATIONS ====\n",
    "# Normalize price\n",
    "scaler = StandardScaler()\n",
    "price_scaled = scaler.fit_transform(df_items[[\"price\"]])\n",
    "\n",
    "# Convert text features to numerical representations\n",
    "df_items[\"text_feature\"] = df_items[\"brand\"] + \" \" + df_items[\"name\"] + \" \" + df_items[\"description\"]\n",
    "\n",
    "# Convert text features to numerical embeddings using simple hashing (alternative to CLIP)\n",
    "df_items[\"text_embedding\"] = df_items[\"text_feature\"].apply(lambda x: hash(x) % (10**8))  # Simple integer hash\n",
    "\n",
    "# Combine all numerical features\n",
    "text_embs = df_items[\"text_embedding\"].to_numpy().reshape(-1, 1)  # (n, 1)\n",
    "price_embs = price_scaled  # (n, 1)\n",
    "\n",
    "# Final item embeddings: Concatenate text hash + price\n",
    "features = np.hstack([text_embs, price_embs])\n",
    "item_embeddings = torch.tensor(features, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 5: CREATE USER EMBEDDINGS ====\n",
    "user_embeddings = {}\n",
    "\n",
    "for user in df_ratings_thresholded.columns:\n",
    "    user_ratings = df_ratings_thresholded[user].dropna()\n",
    "\n",
    "    liked_items = user_ratings[user_ratings == 1].index.tolist()\n",
    "    disliked_items = user_ratings[user_ratings == -1].index.tolist()\n",
    "\n",
    "    liked_items = [idx - 2 for idx in liked_items if 0 <= idx - 2 < len(item_embeddings)]\n",
    "    disliked_items = [idx - 2 for idx in disliked_items if 0 <= idx - 2 < len(item_embeddings)]\n",
    "\n",
    "    if liked_items or disliked_items:\n",
    "        user_embs = []\n",
    "\n",
    "        if liked_items:\n",
    "            user_embs.append(item_embeddings[liked_items].mean(dim=0))\n",
    "\n",
    "        if disliked_items:\n",
    "            user_embs.append(-item_embeddings[disliked_items].mean(dim=0))\n",
    "\n",
    "        user_embeddings[user] = torch.stack(user_embs).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 6: PREPARE TRAINING DATA ====\n",
    "train_pairs = []\n",
    "\n",
    "for user in user_embeddings.keys():\n",
    "    user_ratings = df_ratings_thresholded[user].dropna()\n",
    "\n",
    "    liked_items = user_ratings[user_ratings == 1].index.tolist()\n",
    "    disliked_items = user_ratings[user_ratings == -1].index.tolist()\n",
    "\n",
    "    liked_items = [idx - 2 for idx in liked_items if 0 <= idx - 2 < len(item_embeddings)]\n",
    "    disliked_items = [idx - 2 for idx in disliked_items if 0 <= idx - 2 < len(item_embeddings)]\n",
    "\n",
    "    for item in liked_items:\n",
    "        train_pairs.append((user, item, 1))\n",
    "\n",
    "    for item in disliked_items:\n",
    "        train_pairs.append((user, item, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_text_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m user_text_idx, user_price, item_text_idx, item_price, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create dataset & dataloader\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m UserItemDataset(train_pairs, \u001b[43muser_text_embeddings\u001b[49m, user_price_embeddings, item_text_embeddings, item_price_embeddings)\n\u001b[0;32m     27\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_text_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# ==== STEP 7: CREATE PYTORCH DATASET ====\n",
    "class UserItemDataset(Dataset):\n",
    "    def __init__(self, train_pairs, user_text_data, user_price_data, item_text_data, item_price_data):\n",
    "        self.train_pairs = train_pairs\n",
    "        self.user_text_data = user_text_data\n",
    "        self.user_price_data = user_price_data\n",
    "        self.item_text_data = item_text_data\n",
    "        self.item_price_data = item_price_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.train_pairs[idx]\n",
    "\n",
    "        # Retrieve user and item features\n",
    "        user_text_idx = self.user_text_data[user]  # Tokenized text index\n",
    "        user_price = self.user_price_data[user]\n",
    "\n",
    "        item_text_idx = self.item_text_data[item]  # Tokenized text index\n",
    "        item_price = self.item_price_data[item]\n",
    "\n",
    "        return user_text_idx, user_price, item_text_idx, item_price, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Create dataset & dataloader\n",
    "dataset = UserItemDataset(train_pairs, user_text_embeddings, user_price_embeddings, item_text_embeddings, item_price_embeddings)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50000, text_dim=128, price_dim=1):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "\n",
    "        # Text Embedding Layer (Learned)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, text_dim)\n",
    "\n",
    "        # User Tower\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(text_dim + price_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        # Item Tower\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(text_dim + price_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_text_idx, user_price, item_text_idx, item_price):\n",
    "        # Embed text features\n",
    "        user_text_emb = self.text_embedding(user_text_idx).mean(dim=1)  # Aggregate tokens\n",
    "        item_text_emb = self.text_embedding(item_text_idx).mean(dim=1)\n",
    "\n",
    "        # Concatenate price with text embeddings\n",
    "        user_input = torch.cat([user_text_emb, user_price], dim=1)\n",
    "        item_input = torch.cat([item_text_emb, item_price], dim=1)\n",
    "\n",
    "        # Pass through towers\n",
    "        user_repr = self.user_tower(user_input)\n",
    "        item_repr = self.item_tower(item_input)\n",
    "\n",
    "        # Compute similarity\n",
    "        return F.cosine_similarity(user_repr, item_repr, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TwoTowerModel.forward() missing 2 required positional arguments: 'item_text_idx' and 'item_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_emb, item_emb, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     labels \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(sim, labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TwoTowerModel.forward() missing 2 required positional arguments: 'item_text_idx' and 'item_price'"
     ]
    }
   ],
   "source": [
    "# ==== STEP 9: TRAIN THE MODEL ====\n",
    "model = TwoTowerModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    for user_text_idx, user_price, item_text_idx, item_price, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Pass all required inputs\n",
    "        sim = model(user_text_idx, user_price, item_text_idx, item_price)\n",
    "\n",
    "        # Convert labels from -1/1 to 0/1 for BCE loss\n",
    "        labels = (labels + 1) / 2  \n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(sim, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    losses.append(epoch_loss / len(dataloader))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 10: RECOMMEND ====\n",
    "def recommend_items(user_id, top_k=5):\n",
    "    user_emb = user_embeddings[user_id].unsqueeze(0)\n",
    "    scores = torch.cosine_similarity(model.user_tower(user_emb), model.item_tower(item_embeddings), dim=1)\n",
    "    return df_items.iloc[scores.argsort(descending=True)[:top_k]][[\"brand\", \"name\"]]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
